---
layout: default
title: Apr 21
parent: 2021
grand_parent: Progress Report
---

```txt
04/21 進捗報告　Xiaoran Li

[研究テーマ]
多言語融合による教師なし意義素学習

[現在の目標]
（１）意義素のサンプリング方法を調整する。 
（２）モデルの時間の複雑さを軽減する。 
（３）分散コンピューティングアルゴリズムを統合し、モデルをマージして「エンドツーエンド」を実現する。 
（３）単一言語の意義素学習方法を最適化して統合する。
（４）モデルに反復コンピューティング機能（Iteration）を持たせる。

[今週の実施内容]
(4/15) 水曜日 ： 
	論文(1)から啓発を得て、変分オートエンコーダ（VAE）を使用して反復コンピューティングの機能を実現する可能性を探す。（VAEは生成モデルです。 入力変数の分布を学習し、その分布からサンプルを生成する、 エンコードとデコードは浅いニューラルネットワークです。）自分の思い方：生のテキストのメカニズム（attention）を学習した後、各単語の各意義素のサンプルを見つけ、これらのサンプルをVAEモデルに代入して各単語の意義素分布（mu,sigma）を学習し、次に意義素分布（〜N（mu,sigma^2））からサンプリングして、元の単語（メカニズムしたの単語）生成するそして両方を最小化する。各単語の意義素の分布を学習した後、そのmuは私たちが探している意義素です、sigmaは私たちが探している単語の意義素の重みです。
(4/16) 木曜日 ： 
	簡単のVAEモデルでMNISTを実現した、意義素のVAEモデルを設計する，近年のVAE関連モデルを読む、論文(２)論文(３)。
(4/17) 金曜日 ： 同上
(4/18) 土曜日 ： 同上
(4/19) 日曜日 ： 
	VAEモデルを放棄する，意義素の分散学習の場合、学習では、意義素の分布は多くの高次元のガウス分布で構成されます。 単語の高次元の意義素学習が難い、対応する単語を生成する時、時間計算量は減少せずに増加します。 原因：入力変数は単語ではなく、単語内の意義素であるためです。意義素の場合、1つの単語の意義素分布のみを学習する場合は、VAEを使用できますが、これは少しやり過ぎです。
(4/20) 月曜日 ： 新しいソリューションを探しています。
(4/21) 火曜日 ： 
	見つけだ！この方法は私の最終バージョンになる可能性があります。
	革新：
		1：各単語の意義素の数は、クラスタリングでは学習されません。スパースM次元ベクトルとして設計しました（このベクトルのほとんどはゼロです）。Mは意義素の総数です，これはハイパーパラメータです（例えば：２０００）
		２：古代のクラスタリングおよび分類方法（SVM、K-Meansなど）を放棄し、自分が効率的なクラスタリング手法を設計しました。この方法は長い間学習する必要はありません。単語をスパースなM次元ベクトルにクラスタリングできます（各ベクトルは意義素です）。
		3：これは、元のモデルに基づいて革新的であり、意義素をより意味のあるものにします（単語はこの単語の意義素によって重み付けされ、重みの合計は1に等しくなります。


[今回の実績]
同上


[参考文献]
（１）Neural Text Classification by Jointly Learning to Cluster and Align
（２）Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders
（３）crossing variational Autoencoders for answer retrieval

[次回の目標]
最終バージョンを実現する
```