---
layout: default
title: Jun 2
parent: 2021
grand_parent: Progress Report
---

```txt
06/02 進捗報告　Xiaoran Li

[研究テーマ]
多言語融合による教師なし意義素学習

[現在の目標]
パラメータを繰り返し調整する

[今週の実施内容]
(5/26) 水曜日 ： 研究室（12時間）：コーディング
(5/27) 木曜日 ： 研究室（12時間）：コーディングと授業
(5/28) 金曜日 ： 研究室（13時間）：コーディングと授業
(5/29) 土曜日 ： バイト（8時 間）： 研究室（7時間）：コーディング
(5/30) 日曜日 ： バイト（8時 間）： 研究室（7時間）：コーディング
(5/31) 月曜日 ： 研究室（9時 間）：コーディングと授業
(6/01) 火曜日 ： バイト（8時 間）： 研究室（7時間）：コーディング

[今回の実績]
（１）センテンス学習する方法を調整する
	各センテンスを順番にネットワークに入力して、各単語の意義数を計算する：
		コーパス＝3億個単語, 3千万個センテンスを転換する，最終意義素数＝3億個
		頻度の高い単語が過剰に現れるため、学習した単語の意義数は、頻度の高い単語の意味成分が多くなります。

	コーパス＝3億個単語, 3千万個センテンスで, 一個単語に対してこの単語を含む5000個センテンスを探して、(主に２万個よく現れる単語のセンテンスを探す)：
		(センテンス数 = 2万*5000 = 1億個)、各センテンスは一つ意義素を学習するから、重複ではない。最終意義素数＝1億個
	


（２）時間複雑度 と メモリリーク問題を解決した
	主にNumpy array問題，センテンスサンプリングと義素素捜索問題


（３）Auto-encodingのサイズについて
	MSE Loss:
		もちろん、Latent layer size大きくなれば結果が良い、
		一応ネットワークが:
			Auto-encoding layer:[500,500,2000,10,2000,500,500]
			Input size:768, Batch:128  Learning rate: 0.002 50 epoch で損失関数は落ちていません
			.
			.
			.

			Auto-encoding layer:[500,500,2000,30,2000,500,500]
			Input size:768, Batch:128  Learning rate: 0.001 50 epoch で 40%Batchが全っく学習しいたじゃない。
			.
			.
			.

			Auto-encoding layer:[1000,1000,5000,500,50,500,5000,1000,1000]
			Input size:768, Batch:128  Learning rate: 0.0005 50 epoch で 10%Batchが全っく学習しいたじゃない。
			
			(原因が大体にわかりました、センテンスサンプリング問題です、一応で問題を解決して、単語義素素の捜索数量が上げて、意義素数*５倍をやって見よう)


[参考文献]
特に無い

[次回の目標]
特に無い

```